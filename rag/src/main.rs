use rag::llm::{LlmClient, TongyiClient};
use async_openai::types::{ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs, ChatCompletionRequestUserMessageArgs};
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {

    // åˆ›å»ºé€šä¹‰åƒé—®å®¢æˆ·ç«¯
    let client = TongyiClient::new()
        .with_model("qwen-max".to_string())
        .with_temperature(0.7)
        .with_max_tokens(2000);

    println!("ğŸ¤– é€šä¹‰åƒé—®èŠå¤©æµ‹è¯•\n");

    let messages = vec![
        ChatCompletionRequestMessage::System(
            ChatCompletionRequestSystemMessageArgs::default()
                .content("ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ã€‚")
                .build()?
        ),
        ChatCompletionRequestMessage::User(
            ChatCompletionRequestUserMessageArgs::default()
                .content("Rustè¯­è¨€çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿè¯·ç®€è¦è¯´æ˜ã€‚")
                .build()?
        ),
    ];

    match client.chat(messages).await {
        Ok(response) => {
            println!("âœ… å›å¤: {}\n", response);
        }
        Err(e) => {
            eprintln!("âŒ é”™è¯¯: {}\n", e);
        }
    }

    println!("ğŸ‰ æµ‹è¯•å®Œæˆï¼");

    Ok(())
}
